{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tianbai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import nltk\n",
    "import requests\n",
    "import operator\n",
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load IMDB data\n",
    "path_train_pos = \"IMDb/train/imdb_train_pos.txt\"\n",
    "path_train_neg = \"IMDb/train/imdb_train_neg.txt\"\n",
    "path_test_pos = \"IMDb/test/imdb_test_pos.txt\"\n",
    "path_test_neg = \"IMDb/test/imdb_test_neg.txt\"\n",
    "path_dev_pos = \"IMDb/dev/imdb_dev_pos.txt\"\n",
    "path_dev_neg = \"IMDb/dev/imdb_dev_neg.txt\"\n",
    "\n",
    "dataset_train_pos =open(path_train_pos,'rb').readlines()\n",
    "dataset_train_neg =open(path_train_neg,'rb').readlines()\n",
    "dataset_test_pos =open(path_test_pos,'rb').readlines()\n",
    "dataset_test_neg =open(path_test_neg,'rb').readlines()\n",
    "dataset_dev_pos =open(path_dev_pos,'rb').readlines()\n",
    "dataset_dev_neg =open(path_dev_neg,'rb').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive training data: 7483\n",
      "\n",
      "Number of negative training data: 7517\n",
      "\n",
      "Number of positive testing data: 2499\n",
      "\n",
      "Number of negative testing data: 2501\n",
      "\n",
      "Number of positive develop data: 2518\n",
      "\n",
      "Number of negative develop data: 2482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of positive training data: \"+str(len(dataset_train_pos))+\"\\n\")\n",
    "print (\"Number of negative training data: \"+str(len(dataset_train_neg))+\"\\n\")\n",
    "print (\"Number of positive testing data: \"+str(len(dataset_test_pos))+\"\\n\")\n",
    "print (\"Number of negative testing data: \"+str(len(dataset_test_neg))+\"\\n\")\n",
    "print (\"Number of positive develop data: \"+str(len(dataset_dev_pos))+\"\\n\")\n",
    "print (\"Number of negative develop data: \"+str(len(dataset_dev_neg))+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 15000\n",
      "Size of test set: 5000\n",
      "Size of development set: 5019\n"
     ]
    }
   ],
   "source": [
    "train_set=[]\n",
    "test_set=[]\n",
    "dev_set=[]\n",
    "\n",
    "# combine positive and negitive review together as train set and test set\n",
    "# and decode byte data to string\n",
    "for pos_review in dataset_train_pos:\n",
    "  pos_review_str = pos_review.decode();\n",
    "  train_set.append((pos_review_str,1))\n",
    "for neg_review in dataset_train_neg:\n",
    "  neg_review_str = neg_review.decode(); \n",
    "  train_set.append((neg_review_str,0))\n",
    "random.shuffle(train_set)\n",
    "\n",
    "for pos_review in dataset_test_pos:\n",
    "  pos_review_str = pos_review.decode();\n",
    "  test_set.append((pos_review_str,1))\n",
    "for neg_review in dataset_test_neg:\n",
    "  neg_review_str = neg_review.decode();\n",
    "  test_set.append((neg_review_str,0))\n",
    "random.shuffle(test_set)\n",
    "\n",
    "\n",
    "for pos_review in dataset_dev_pos:\n",
    "  pos_review_str = pos_review.decode();\n",
    "  dev_set.append((pos_review_str,1))\n",
    "for neg_review in dataset_test_neg:\n",
    "  neg_review_str = neg_review.decode();\n",
    "  dev_set.append((neg_review_str,0))\n",
    "random.shuffle(dev_set)\n",
    "\n",
    "\n",
    "print (\"Size of training set: \"+str(len(train_set)))\n",
    "print (\"Size of test set: \"+str(len(test_set)))\n",
    "print (\"Size of development set: \"+str(len(dev_set)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def get_list_tokens(string):\n",
    "  sentence_split=nltk.tokenize.sent_tokenize(string)\n",
    "  list_tokens=[]\n",
    "  for sentence in sentence_split:\n",
    "    list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n",
    "    for token in list_tokens_sentence:\n",
    "      list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "  return list_tokens\n",
    "\n",
    "\n",
    "# First, we get the stopwords list from nltk\n",
    "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
    "# We can add more words to the stopword list, like punctuation marks\n",
    "stopwords.add(\".\")\n",
    "stopwords.add(\",\")\n",
    "stopwords.add(\"--\")\n",
    "stopwords.add(\"``\")\n",
    "stopwords.add(\"#\")\n",
    "stopwords.add(\"@\")\n",
    "stopwords.add(\":\")\n",
    "stopwords.add(\"1\")\n",
    "stopwords.add(\"0\")\n",
    "stopwords.add(\"/\")\n",
    "stopwords.add(\">\")\n",
    "stopwords.add(\"<\")\n",
    "stopwords.add(\"(\")\n",
    "stopwords.add(\")\")\n",
    "\n",
    "\n",
    "# Now we create a frequency dictionary with all words in the dataset\n",
    "\n",
    "\n",
    "def get_list_tokens(string): # Function to retrieve the list of tokens from a string\n",
    "  sentence_split=nltk.tokenize.sent_tokenize(string)\n",
    "  list_tokens=[]\n",
    "  for sentence in sentence_split:\n",
    "    list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n",
    "    for token in list_tokens_sentence:\n",
    "      list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "  return list_tokens\n",
    "\n",
    "\n",
    "def get_list_ADJ_VERB_tokens(string): # Function to retrieve the list of tokens from a string\n",
    "  sentence_split=nltk.tokenize.sent_tokenize(string)\n",
    "  list_tokens=[]\n",
    "  for sentence in sentence_split:\n",
    "    list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n",
    "    list_tokens_sentence_tag =nltk.pos_tag(list_tokens_sentence);\n",
    "    #print(list_tokens_sentence_tag)\n",
    "    for token,tag in list_tokens_sentence_tag:\n",
    "      if tag =='ADJ' or tag =='VB' or tag =='VBP' or tag =='VBG' :\n",
    "          list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "  return list_tokens\n",
    "\n",
    "\n",
    "def get_vector_text(list_vocab,string):\n",
    "  vector_text=np.zeros(len(list_vocab))\n",
    "  list_tokens_string=get_list_tokens(string)\n",
    "  for i, word in enumerate(list_vocab):\n",
    "    if word in list_tokens_string:\n",
    "      vector_text[i]=list_tokens_string.count(word)\n",
    "  return vector_text\n",
    "\n",
    "\n",
    "def get_vocabulary(training_set, num_features, type_features): # Function to retrieve vocabulary\n",
    "  dict_word_frequency={}\n",
    "  for instance in training_set:\n",
    "    sentence_tokens = []\n",
    "    if type_features == \"totalWordsFrequency\":\n",
    "        sentence_tokens=get_list_tokens(instance[0])#for feature type 1\n",
    "    if type_features == \"Adj_VerbWordsFrequency\":\n",
    "        sentence_tokens=get_list_ADJ_VERB_tokens(instance[0])#for feature type 2\n",
    "    else:\n",
    "        sentence_tokens=get_list_tokens(instance[0])#default\n",
    "    for word in sentence_tokens:\n",
    "      if word in stopwords: continue\n",
    "      if word not in dict_word_frequency: dict_word_frequency[word]=1\n",
    "      else: dict_word_frequency[word]+=1\n",
    "  sorted_list = sorted(dict_word_frequency.items(), key=operator.itemgetter(1), reverse=True)[:num_features]\n",
    "  vocabulary=[]\n",
    "  for word,frequency in sorted_list:\n",
    "    vocabulary.append(word)\n",
    "  i=0  \n",
    "  for word,frequency in sorted_list[:10]:\n",
    "      i+=1\n",
    "      print (str(i)+\". \"+word+\" - \"+str(frequency))\n",
    "  return vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. br - 59548\n",
      "2. 's - 36104\n",
      "3. movie - 29648\n",
      "4. wa - 29577\n",
      "5. film - 26929\n",
      "6. '' - 19857\n",
      "7. n't - 19640\n",
      "8. one - 15987\n",
      "9. ! - 14847\n",
      "10. like - 11876\n",
      "total num of features:1000\n"
     ]
    }
   ],
   "source": [
    "# add features of the total frequency of words to vocabulary\n",
    "vocabulary=get_vocabulary(train_set, 1000,\"totalWordsFrequency\")  # We use the get_vocabulary function to retrieve the vocabulary\n",
    "print(\"total num of features:\"+str(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. see - 6605\n",
      "2. get - 5218\n",
      "3. make - 4502\n",
      "4. think - 4141\n",
      "5. know - 3735\n",
      "6. watch - 3619\n",
      "7. say - 3153\n",
      "8. 've - 2974\n",
      "9. 'm - 2795\n",
      "10. watching - 2602\n",
      "total num of features:2000\n"
     ]
    }
   ],
   "source": [
    "# add features of the frequency of ADJ and VERB words to vocabulary,and this can take a while...\n",
    "vocabulary.extend(get_vocabulary(train_set, 1000,\"Adj_VerbWordsFrequency\"))  \n",
    "print(\"total num of features:\"+str(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total num of features:3000\n"
     ]
    }
   ],
   "source": [
    "#Add features of key words to vocabulary, by using CountVectorizer \n",
    "\n",
    "X_train=[]\n",
    "\n",
    "for instance in train_set:\n",
    "    X_train.append(instance[0])\n",
    "  \n",
    "vectorizer = CountVectorizer(max_features = 1000)\n",
    "X = vectorizer.fit_transform(X_train)\n",
    "word = vectorizer.get_feature_names()\n",
    "vocabulary.extend(word)  \n",
    "print(\"total num of features:\"+str(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size original training matrix: (15000, 3000)\n",
      "Size new training matrix: (15000, 1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce dimension from 3000 to 1000 and train SVM classifier. This can take for a long time...\n",
    "\n",
    "X_train=[]\n",
    "Y_train=[]\n",
    "for instance in train_set:\n",
    "    vector_instance=get_vector_text(vocabulary,instance[0])\n",
    "    X_train.append(vector_instance)\n",
    "    Y_train.append(instance[1])\n",
    "    \n",
    "X_train = np.asarray(X_train)\n",
    "Y_train = np.asarray(Y_train) \n",
    "    \n",
    "    \n",
    "fs_sentanalysis=SelectKBest(chi2, k=1000).fit(X_train, Y_train)\n",
    "X_train_new = fs_sentanalysis.transform(X_train)   \n",
    "\n",
    "print (\"Size original training matrix: \"+str(X_train.shape))\n",
    "print (\"Size new training matrix: \"+str(X_train_new.shape))  \n",
    "    \n",
    "# Finally, we train the SVM classifier \n",
    "svm_clf=svm.SVC(kernel=\"linear\",gamma='auto')\n",
    "svm_clf.fit(X_train_new,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.84      0.85      2501\n",
      "           1       0.85      0.87      0.86      2518\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      5019\n",
      "   macro avg       0.85      0.85      0.85      5019\n",
      "weighted avg       0.85      0.85      0.85      5019\n",
      "\n",
      "accuracy :0.8539549711097828\n"
     ]
    }
   ],
   "source": [
    "# check the performance of development set firstly, this is for fine tuning\n",
    "\n",
    "X_dev=[]\n",
    "Y_dev=[]\n",
    "for instance in dev_set:\n",
    "  vector_instance=get_vector_text(vocabulary,instance[0])\n",
    "  X_dev.append(vector_instance)\n",
    "  Y_dev.append(instance[1])\n",
    "X_dev=np.asarray(X_dev)\n",
    "Y_dev_gold=np.asarray(Y_dev)\n",
    "Y_dev_predictions=svm_clf.predict(fs_sentanalysis.transform(X_dev))\n",
    "print(classification_report(Y_dev_gold, Y_dev_predictions))\n",
    "print(\"accuracy :\"+str(accuracy_score(Y_dev_gold, Y_dev_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.85      2501\n",
      "           1       0.85      0.87      0.86      2499\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      5000\n",
      "   macro avg       0.86      0.86      0.86      5000\n",
      "weighted avg       0.86      0.86      0.86      5000\n",
      "\n",
      "accuracy :0.8564\n"
     ]
    }
   ],
   "source": [
    "# check the performance of test dataset\n",
    "X_test=[]\n",
    "Y_test=[]\n",
    "for instance in test_set:\n",
    "  vector_instance=get_vector_text(vocabulary,instance[0])\n",
    "  X_test.append(vector_instance)\n",
    "  Y_test.append(instance[1])\n",
    "X_test=np.asarray(X_test)\n",
    "Y_test_gold=np.asarray(Y_test)\n",
    "Y_test_predictions=svm_clf.predict(fs_sentanalysis.transform(X_test))\n",
    "print(classification_report(Y_test_gold, Y_test_predictions))\n",
    "print(\"accuracy :\"+str(accuracy_score(Y_test_gold, Y_test_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
